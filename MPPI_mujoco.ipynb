{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPPI():\n",
    "    \"\"\" MMPI according to algorithm 2 in Williams et al., 2017\n",
    "        'Information Theoretic MPC for Model-Based Reinforcement Learning' \"\"\"\n",
    "\n",
    "    def __init__(self, env, K, T, U, lambda_=1.0, noise_mu=0, noise_sigma=1, u_init=1, gamma=0.99, log_file=None, noise_gaussian=True, downward_start=True):\n",
    "        self.K = K  # N_SAMPLES\n",
    "        self.T = T  # TIMESTEPS\n",
    "        self.lambda_ = lambda_\n",
    "        self.noise_mu = noise_mu\n",
    "        self.noise_sigma = noise_sigma\n",
    "        self.U = U\n",
    "        self.u_init = u_init\n",
    "        self.reward_total = np.zeros(shape=(self.K))\n",
    "\n",
    "        self.env = env\n",
    "        self.env.reset()\n",
    "        if downward_start:\n",
    "            self.env.env.state = [np.pi, 1]\n",
    "        ############################\n",
    "        if self.env.unwrapped.spec.id == \"Pendulum-v0\":\n",
    "            self.x_init = self.env.env.state\n",
    "        elif self.env.unwrapped.spec.id == \"HumanoidStandup-v2\":\n",
    "            self.x_init = env.sim.get_state()\n",
    "        ############################\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.log_file = log_file\n",
    "        if log_file is not None:\n",
    "            self.writer = csv.writer(log_file, delimiter='\\t')\n",
    "            headers = [\"timestamp\", \"reward\", \"action\", \"state\"]\n",
    "            self.writer.writerow(headers)\n",
    "        \n",
    "\n",
    "        if noise_gaussian:\n",
    "            self.noise = np.random.normal(loc=self.noise_mu, scale=self.noise_sigma, size=(self.K, self.T, self.env.action_space.shape[0]))\n",
    "        else:\n",
    "            self.noise = np.full(shape=(self.K, self.T), fill_value=0.9)\n",
    "\n",
    "    def _get_reward_from_state(self, s):\n",
    "        root_z = s[0]\n",
    "        if root_z > 1.1:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 1.0 - (1.1 - root_z)\n",
    "    \n",
    "    def _compute_total_reward(self, k):\n",
    "        discount = 1\n",
    "        ############################\n",
    "        if self.env.unwrapped.spec.id == \"Pendulum-v0\":\n",
    "            self.env.env.state = self.x_init\n",
    "        elif self.env.unwrapped.spec.id == \"HumanoidStandup-v2\":\n",
    "            self.env.sim.set_state(self.x_init)\n",
    "        ############################\n",
    "        for t in range(self.T):\n",
    "            perturbed_action_t = self.U[t] + self.noise[k, t]\n",
    "            s, reward, _, _ = self.env.step(np.array([perturbed_action_t]))\n",
    "            if self.env.unwrapped.spec.id == \"HumanoidStandup-v2\":\n",
    "                reward = self._get_reward_from_state(s)\n",
    "            self.reward_total[k] += discount * reward\n",
    "            discount *= self.gamma\n",
    "\n",
    "    def _ensure_non_zero(self, reward, beta, factor):\n",
    "        return np.exp(-factor * (beta - reward))\n",
    "\n",
    "\n",
    "    def control(self, iter=1000):\n",
    "        for timestamp in range(iter):\n",
    "            for k in range(self.K):\n",
    "                self._compute_total_reward(k)\n",
    "\n",
    "            beta = np.max(self.reward_total)  # maximum reward of all trajectories\n",
    "#             print()\n",
    "#             print(self.reward_total)\n",
    "#             print(beta)\n",
    "            reward_total_non_zero = self._ensure_non_zero(reward=self.reward_total, beta=beta, factor=1/self.lambda_)\n",
    "#             print(reward_total_non_zero)\n",
    "            eta = np.sum(reward_total_non_zero)\n",
    "            \n",
    "            omega = 1/eta * reward_total_non_zero\n",
    "#             print(\"Omega: {}\".format(omega))\n",
    "#             print(\"Noise: {}\".format(self.noise))\n",
    "#             print(\"U before: {}\".format(self.U))\n",
    "            self.U += [np.sum(omega.reshape(len(omega), 1) * self.noise[:, t], axis=0) for t in range(self.T)]\n",
    "#             print(\"Incremental: {}\".format([np.sum(omega.reshape(len(omega), 1) * self.noise[:, t], axis=0) for t in range(self.T)]))\n",
    "#             print(\"U after: {}\".format(self.U))\n",
    "            ############################\n",
    "            if self.env.unwrapped.spec.id == \"Pendulum-v0\":\n",
    "                self.env.env.state = self.x_init\n",
    "            elif self.env.unwrapped.spec.id == \"HumanoidStandup-v2\":\n",
    "                self.env.sim.set_state(self.x_init)\n",
    "            ############################\n",
    "            s, r, _, _ = self.env.step(np.array([self.U[0]]))\n",
    "            try:\n",
    "                r = r[0]\n",
    "            except:\n",
    "                pass\n",
    "            if self.env.unwrapped.spec.id == \"HumanoidStandup-v2\":\n",
    "                r = self._get_reward_from_state(s)\n",
    "            print(\"timestamp: {}, action taken: {} reward received: {}\".format(timestamp, self.U[0], r))\n",
    "            self.env.render()\n",
    "#             self.env.sim.render(1024, 1024)\n",
    "\n",
    "            self.U = np.roll(self.U, -1, axis=0)\n",
    "\n",
    "            self.U[-1] = self.u_init  #\n",
    "            self.reward_total[:] = 0\n",
    "#             print(\"U after shifting: {}\".format(self.U))\n",
    "#             print(\"Rewards reset: {}\".format(self.reward_total))\n",
    "            \n",
    "            ############################\n",
    "            if self.env.unwrapped.spec.id == \"Pendulum-v0\":\n",
    "                self.x_init = self.env.env.state\n",
    "            elif self.env.unwrapped.spec.id == \"HumanoidStandup-v2\":\n",
    "                self.x_init = self.env.sim.get_state()\n",
    "            ###########################\n",
    "            \n",
    "            if self.writer is not None:\n",
    "                self._write_record(timestamp, r, self.U[0], s)\n",
    "            \n",
    "            self.noise = np.random.normal(loc=self.noise_mu, scale=self.noise_sigma, size=(self.K, self.T, self.env.action_space.shape[0]))\n",
    "    \n",
    "    def _write_record(self, timestamp, reward, action, state):\n",
    "        action_json = json.dumps(action.tolist())\n",
    "        state_json = json.dumps(state.reshape(len(state), 1).tolist())\n",
    "        self.writer.writerow([timestamp, reward, action_json, state_json])\n",
    "        self.log_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(376,)\n",
      "Box(17,)\n",
      "timestamp: 0, action taken: [-0.05589273 -0.44398878 -0.2168678   0.12353883  0.82782733  0.7227695\n",
      "  0.64459922 -0.75122054  0.50335441  0.40465551  0.10404981  0.91825837\n",
      " -0.46075942  0.38765135 -0.88638594 -0.79940167 -0.41552672] reward received: 0.009915006752120759\n",
      "Creating window glfw\n",
      "timestamp: 1, action taken: [-0.84774226 -0.58755453 -0.00534756 -0.83660643  0.49206494  0.01314316\n",
      " -0.54842138 -0.92437468 -0.15628649  1.04437873  0.19436964  0.65239101\n",
      " -0.63267831 -0.97833637 -0.3844731  -0.05775615  1.02283048] reward received: 0.00558747580792518\n",
      "timestamp: 2, action taken: [-0.80802571 -0.51550129  0.53985051 -1.09483317 -0.81791757 -0.76350256\n",
      "  0.27097373  0.28012019  0.22387869  0.08907796  0.1006052   0.96910197\n",
      "  0.14520837  0.10373177 -0.11901161  0.0156499   0.94104786] reward received: 0.0027498242958787777\n",
      "timestamp: 3, action taken: [ 0.7009908   0.66348932  0.92300016  0.45183259  1.04702566  0.84339431\n",
      "  0.83227973  0.73807801 -0.75323916 -0.40039056  0.28339114 -0.31325019\n",
      "  0.03519488  0.87761347 -0.38436429  0.75088967 -0.84029045] reward received: 0.0009712756252052701\n",
      "timestamp: 4, action taken: [-0.6751062  -0.61470914 -0.81522207 -0.74855248  0.43643077 -0.2927535\n",
      "  0.90934574  0.60621338 -0.11321897  0.50135943  1.03966921 -1.07142329\n",
      " -0.00892989  0.57241028  0.4863941  -0.53335002 -0.69950257] reward received: -0.003773398616224277\n",
      "timestamp: 5, action taken: [ 0.3033412   0.09886821  0.55700025 -0.75950011 -0.91652832 -0.8740063\n",
      " -0.39126472  0.1483485  -0.04476661 -0.34592681 -0.68123212 -0.47409248\n",
      " -0.24336544  0.14606366 -0.06951383 -0.59220374 -0.2551066 ] reward received: -0.006969070648599196\n",
      "timestamp: 6, action taken: [-0.29610502  0.449927    0.55579903  0.86106842 -0.31191693  0.60205311\n",
      " -0.56335904 -0.43361554 -0.23354611  0.56369305 -0.58226612  0.63022283\n",
      " -0.1279532   0.0886131  -0.19979072  0.55361172 -0.030314  ] reward received: -0.012303533211698081\n",
      "timestamp: 7, action taken: [-0.29513759 -0.58018127 -1.14437131  0.27238416 -0.89819196 -0.79031806\n",
      " -0.17473029  0.5262984   0.86156771 -0.54046993  0.30701024  0.91291129\n",
      "  0.78675912 -0.62664638 -0.70925496 -1.15756051 -0.430839  ] reward received: -0.017721727856964797\n",
      "timestamp: 8, action taken: [-0.26778738  0.07196825 -0.02268224 -0.10143124 -0.06038435  0.29188454\n",
      " -0.13135302  0.06760952  0.22268472  0.14546741 -0.23220781 -0.02785091\n",
      "  0.04371018 -0.09401381  0.2146087   0.05810345 -0.12684671] reward received: -0.019325483692659073\n",
      "timestamp: 9, action taken: [ 0.61630006  0.35140815 -0.11571548  0.12820554 -0.3315      0.04156042\n",
      " -0.02108513  0.0991921   0.120611   -0.11123496  0.10508891  0.07126292\n",
      "  0.17628247  0.02312532 -0.23632967  0.37139226  0.22688272] reward received: -0.020902792762439004\n",
      "timestamp: 10, action taken: [-0.1044981  -0.55737173  0.30046789 -0.37638193  0.11068761  0.03895433\n",
      " -0.21249091  0.20479627 -0.20737641 -0.13954418 -0.57375861  0.25662782\n",
      " -0.35379093  0.20842196  0.08058544 -0.19906684 -0.16006624] reward received: -0.020354768189368544\n",
      "timestamp: 11, action taken: [ 0.02630495  0.006587   -0.14433084 -0.11180736  0.41973585 -0.19228126\n",
      "  0.05561671 -0.40296626  0.24990609 -0.17286     0.08162492 -0.11456602\n",
      " -0.20072156  0.18850498  0.16232429 -0.15058309  0.3888425 ] reward received: -0.018933250205808827\n",
      "timestamp: 12, action taken: [ 0.14487696 -0.20943582  0.2032246  -0.04792385 -0.40725619  0.43524473\n",
      " -0.29909866  0.07124211  0.27428082 -0.05548666  0.10669648  0.005425\n",
      " -0.24589127 -0.06338179  0.33274263 -0.55386991 -0.0669083 ] reward received: -0.020246978222358658\n",
      "timestamp: 13, action taken: [-0.32529055 -0.56944507  0.25720312  0.18683987  0.18140508 -0.18290932\n",
      " -0.26020725 -0.43444661  0.27899241  0.83420528 -0.28979841  0.24831477\n",
      " -0.0190779  -0.09732026 -0.2087097   0.22394299  0.50214278] reward received: -0.021542679310572188\n",
      "timestamp: 14, action taken: [-0.06706589 -0.09007329 -0.20518781  0.04976343 -0.05604654 -0.38197938\n",
      " -0.25608391  0.053653   -0.23698589 -0.41590434  0.34116476 -0.06021775\n",
      " -0.28369025  0.61858633  0.19778416 -0.45924794  0.25830453] reward received: -0.015609801132763668\n",
      "timestamp: 15, action taken: [ 0.09621733 -0.01985902 -0.0928634  -0.34540471  0.46383167  0.26049024\n",
      " -0.06737839 -0.17443518  0.41114636 -0.42683874  0.08735524  0.4482628\n",
      "  0.05217156  0.09526125 -0.35482161  0.46661768 -0.06550338] reward received: -0.009630846795105397\n",
      "timestamp: 16, action taken: [ 0.04941739  0.35000175 -0.01412179 -0.03038098  0.03999222  0.27058182\n",
      "  0.38083125  0.16298814 -0.21832877 -0.28809469 -0.09958971  0.45052392\n",
      "  0.32441408 -0.19159177  0.10925533 -0.17102907  0.05833286] reward received: -0.0075477260842113925\n",
      "timestamp: 17, action taken: [ 0.10066829  0.35950785  0.1317364  -0.20129643  0.14338161 -0.19708762\n",
      "  0.04009702  0.26292342 -0.3930828  -0.22276741 -0.08411501 -0.40700562\n",
      " -0.14982787  0.0543598  -0.07850022  0.11739822  0.0808619 ] reward received: -0.0061775057926711785\n",
      "timestamp: 18, action taken: [-0.4313396  -0.61992901 -0.17281688  0.01471526  0.20402964 -0.12400769\n",
      " -0.11365802  0.03243864 -0.3788856   0.25920861  0.12144181 -0.06794172\n",
      "  0.18371574  0.4923521   0.12234742 -0.27782801  0.11692568] reward received: -0.005919165849146202\n",
      "timestamp: 19, action taken: [ 0.20893872  0.0722161   0.23602491 -0.15225578  0.42029211 -0.277688\n",
      " -0.24704405 -0.08378701  0.20877386 -0.33815502  0.06063178  0.04463825\n",
      "  0.2160351   0.04836159  0.02338318  0.32324268 -0.02102367] reward received: -0.004710999307576014\n",
      "timestamp: 20, action taken: [ 0.02183852  0.24219382 -0.15993618  0.00832993 -0.20223792  0.49555192\n",
      " -0.10317901 -0.18000189  0.06418986 -0.10320361 -0.26291732  0.019644\n",
      "  0.03556357 -0.01317831  0.16846695  0.14314364 -0.5091678 ] reward received: -0.005608532855224668\n",
      "timestamp: 21, action taken: [-0.0872435   0.26421291  0.03234213  0.00580657 -0.15840839  0.07113405\n",
      " -0.11465221  0.02876986  0.00815429 -0.46083093 -0.08250238 -0.11276598\n",
      "  0.25389917 -0.10767759 -0.15646886  0.17279968  0.35576631] reward received: -0.007567297148547869\n",
      "timestamp: 22, action taken: [ 0.37592113  0.27826541 -0.08618835 -0.23363859  0.24954     0.25091644\n",
      "  0.10270152 -0.24863248  0.33725314  0.14398203  0.14879417  0.1165675\n",
      " -0.15455118  0.06124725  0.03230409 -0.33904869  0.39921907] reward received: -0.012300794793029368\n",
      "timestamp: 23, action taken: [-0.1245954  -0.0893939  -0.47548934 -0.45759054 -0.02478484  0.11888024\n",
      " -0.39203148 -0.29674601 -0.34746328  0.31924651 -0.37691228  0.33067396\n",
      "  0.16594515 -0.06318202 -0.01101414  0.03355788  0.50581021] reward received: -0.018408856224410686\n",
      "timestamp: 24, action taken: [ 0.05674837 -0.3422803  -0.15132021 -0.45558216  0.2245756  -0.38634826\n",
      " -0.16848329  0.0846346  -0.35423336 -0.34646031  0.17432974  0.40361753\n",
      "  0.17335335 -0.0793647   0.26923848  0.22659784  0.03023252] reward received: -0.014939920250353067\n",
      "timestamp: 25, action taken: [-0.25109429  0.11347108  0.02973158  0.09120313 -0.30502764 -0.08878831\n",
      " -0.10117045 -0.05459304  0.09154302  0.07242491 -0.03022586 -0.15792273\n",
      " -0.0422243  -0.18863915 -0.09747022 -0.20412897 -0.17105983] reward received: -0.010817950389206743\n",
      "timestamp: 26, action taken: [-0.1205987   0.08593693  0.35493083  0.40340517 -0.0663318  -0.18729354\n",
      " -0.15176506 -0.01038116  0.06517189  0.14504762 -0.18443495  0.21161649\n",
      "  0.01103423 -0.4516563   0.13421658 -0.33156976 -0.68289598] reward received: -0.009585912839275679\n",
      "timestamp: 27, action taken: [-0.21200933  0.20759642 -0.27255054  0.19410353  0.13669922 -0.21946039\n",
      "  0.6274926  -0.29616298 -0.10160861  0.25087037 -0.02023531  0.18495957\n",
      "  0.26321572  0.14567238 -0.12319001  0.02385418 -0.05225815] reward received: -0.010061418980905845\n",
      "timestamp: 28, action taken: [-0.32488123 -0.01171866  0.0278172   0.27074655  0.36156317 -0.04213519\n",
      " -0.26418501 -0.01584967  0.11619267  0.52269303  0.28312007  0.04615895\n",
      " -0.36677029 -0.14539542 -0.09233302  0.20578257 -0.32915256] reward received: -0.013672151879660088\n",
      "timestamp: 29, action taken: [-0.25433695  0.32714689 -0.55640065  0.10698933 -0.01459546  0.12271322\n",
      " -0.07117458  0.10854799  0.26012941 -0.19262652  0.45870905  0.37539754\n",
      " -0.02581047 -0.3342548   0.03664532 -0.05789175 -0.28521684] reward received: -0.01595413880760388\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"HumanoidStandup-v2\"\n",
    "TIMESTEPS = 64  # T\n",
    "N_SAMPLES = 128  # K\n",
    "ACTION_LOW = -1.0\n",
    "ACTION_HIGH = 1.0\n",
    "\n",
    "# TIMESTEPS = 15 # T\n",
    "# N_SAMPLES = 120  # K\n",
    "\n",
    "noise_mu = 0\n",
    "noise_sigma = 0.2\n",
    "lambda_ = 1.25\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "# from gym.wrappers import Monitor\n",
    "# env = Monitor(env, './video', force=True)\n",
    "# env._max_episode_steps = 200\n",
    "# env.render()\n",
    "# env.sim.render(1024, 1024)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "U = np.random.uniform(low=ACTION_LOW, high=ACTION_HIGH, size=(TIMESTEPS, env.action_space.shape[0]))  # pendulum joint effort in (-2, +2)\n",
    "# print(U)\n",
    "\n",
    "log_file = open(\"train_record_tmp.tsv\", \"w\")\n",
    "\n",
    "\n",
    "mppi_gym = MPPI(env=env, K=N_SAMPLES, T=TIMESTEPS, U=U, lambda_=lambda_, noise_mu=noise_mu, noise_sigma=noise_sigma, u_init=0, gamma=gamma, log_file=log_file, noise_gaussian=True)\n",
    "mppi_gym.control(iter=30)\n",
    "\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"HumanoidStandup-v2\"\n",
    "TIMESTEPS = 8  # T\n",
    "N_SAMPLES = 4  # K\n",
    "ACTION_LOW = -1.0\n",
    "ACTION_HIGH = 1.0\n",
    "\n",
    "# TIMESTEPS = 15 # T\n",
    "# N_SAMPLES = 120  # K\n",
    "\n",
    "noise_mu = 0\n",
    "noise_sigma = 0.2\n",
    "lambda_ = 1.25\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "s = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n"
     ]
    }
   ],
   "source": [
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(HumanoidStandup-v2)\n"
     ]
    }
   ],
   "source": [
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = dir(__builtins__)\n",
    "d = __builtins__.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_elapsed_steps': 0,\n",
      " '_episode_started_at': 1556930514.5379677,\n",
      " '_max_episode_seconds': None,\n",
      " '_max_episode_steps': 1000,\n",
      " 'action_space': Box(17,),\n",
      " 'env': <gym.envs.mujoco.humanoidstandup.HumanoidStandupEnv object at 0x7ff7b0516c18>,\n",
      " 'metadata': {'render.modes': ['human', 'rgb_array', 'depth_array'],\n",
      "              'video.frames_per_second': 67},\n",
      " 'observation_space': Box(376,),\n",
      " 'reward_range': (-inf, inf),\n",
      " 'spec': EnvSpec(HumanoidStandup-v2)}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(vars(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (polo)",
   "language": "python",
   "name": "polo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
